{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SignVerification_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aftaab99/OfflineSignatureVerification/blob/master/SignVerification_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZT1ApwekgYh",
        "colab_type": "code",
        "outputId": "34397270-18ad-4e02-fe62-7167555e6924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "# Installing dependencies and mounting Google drive\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install -U scikit-learn\n",
        "!pip install --no-cache-dir -I pillow\n",
        "\n",
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' #'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print('Platform:', platform, 'Accelerator:', accelerator)\n",
        "\n",
        "!pip install --upgrade --force-reinstall -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl\n",
        "\n",
        "import torch\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.3)\n",
            "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.21.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.12.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.16.3)\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/c2/f84b1e57416755e967236468dcfb0fad7fd911f707185efc4ba8834a1a94/Pillow-6.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 6.9MB/s \n",
            "\u001b[31mERROR: fastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-6.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Platform: cp36-cp36m Accelerator: cu80\n",
            "\u001b[K     |████████████████████████████████| 484.0MB 52.2MB/s \n",
            "\u001b[31mERROR: fastai 1.0.52 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hTorch 0.4.0 CUDA 8.0.61\n",
            "Device: cuda:0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhv2QcpRmLzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import Linear, Conv2d, MaxPool2d, LocalResponseNorm, Dropout\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn import Module\n",
        "from PIL import Image\n",
        "from PIL.ImageOps import invert\n",
        "import numpy as np\n",
        "from torch.tensor import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from random import randrange\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from torch import save\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zlB7PkzkgYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing and Dataloaders\n",
        "\n",
        "\n",
        "def invert_image(path):\n",
        "\timage_file = Image.open(path) # open colour image\n",
        "\timage_file = image_file.convert('L').resize([220, 155])\n",
        "\timage_file = invert(image_file)\n",
        "\timage_array = np.array(image_file)\n",
        "\tfor i in range(image_array.shape[0]):\n",
        "\t\tfor j in range(image_array.shape[1]):\n",
        "\t\t\tif image_array[i][j]<=50:\n",
        "\t\t\t\timage_array[i][j]=0\n",
        "\t\t\telse:\n",
        "\t\t\t\timage_array[i][j]=255\n",
        "\treturn image_array\n",
        "\n",
        "def convert_to_image_tensor(image_array):\n",
        "\timage_array = image_array/255.0\n",
        "\treturn Tensor(image_array).view(1, 220, 155)\n",
        "\n",
        "base_path_org = 'CEDAR signature verification/full_org/original_%d_%d.png'\n",
        "base_path_forg = 'CEDAR signature verification/full_forg/forgeries_%d_%d.png'\n",
        "\n",
        "def fix_pair(x, y):\n",
        "\tif x == y:\n",
        "\t\treturn fix_pair(x, randrange(1, 24))\n",
        "\telse:\n",
        "\t\treturn x, y\n",
        "\n",
        "data = []\n",
        "n_samples_of_each_class = 900\n",
        "\n",
        "prefix ='/content/drive/My Drive/'\n",
        "\n",
        "for _ in range(n_samples_of_each_class):\n",
        "\tanchor_person = randrange(1, 55)\n",
        "\tanchor_sign = randrange(1, 24)\n",
        "\tpos_sign = randrange(1, 24)\n",
        "\tanchor_sign, pos_sign = fix_pair(anchor_sign, pos_sign)\n",
        "\tneg_sign = randrange(1, 24)\n",
        "\tpositive = [base_path_org%(anchor_person, anchor_sign), base_path_org%(anchor_person, pos_sign), 1]\n",
        "\tnegative = [base_path_org%(anchor_person, anchor_sign), base_path_forg%(anchor_person, neg_sign), 0]\n",
        "\tdata.append(positive)\n",
        "\tdata.append(negative)\n",
        "\n",
        "\n",
        "train, test = train_test_split(data, test_size=0.15)\n",
        "with open('train_index.pkl', 'wb') as train_index_file:\n",
        "\tpickle.dump(train, train_index_file)\n",
        "\n",
        "with open('test_index.pkl', 'wb') as test_index_file:\n",
        "\tpickle.dump(test, test_index_file)\n",
        "\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\twith open('train_index.pkl', 'rb') as train_index_file:\n",
        "\t\t\tself.pairs = pickle.load(train_index_file)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.pairs[index]\n",
        "\t\tX = convert_to_image_tensor(invert_image(prefix+item[0]))\n",
        "\t\tY = convert_to_image_tensor(invert_image(prefix+item[1]))\n",
        "\t\treturn [X, Y, item[2]]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.pairs)\n",
        "\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\twith open('test_index.pkl', 'rb') as test_index_file:\n",
        "\t\t\tself.pairs = pickle.load(test_index_file)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.pairs[index]\n",
        "\t\tX = convert_to_image_tensor(invert_image(prefix+item[0]))\n",
        "\t\tY = convert_to_image_tensor(invert_image(prefix+item[1]))\n",
        "\t\treturn [X, Y, item[2]]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.pairs)\n",
        "  \n",
        "\n",
        "class SiameseConvNet(Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.conv1 = Conv2d(1, 48, kernel_size=(11, 11), stride=1)\n",
        "\t\tself.lrn1 = LocalResponseNorm(48, alpha=1e-4, beta=0.75, k=2)\n",
        "\t\tself.pool1 = MaxPool2d(kernel_size=(3, 3), stride=2)\n",
        "\t\tself.conv2 = Conv2d(48, 128, kernel_size=(5, 5), stride=1, padding=2)\n",
        "\t\tself.lrn2 = LocalResponseNorm(128, alpha=1e-4, beta=0.75, k=2)\n",
        "\t\tself.pool2 = MaxPool2d(kernel_size=(3, 3), stride=2)\n",
        "\t\tself.dropout1 = Dropout(0.3)\n",
        "\t\tself.conv3 = Conv2d(128, 256, kernel_size=(3, 3), stride=1, padding=1)\n",
        "\t\tself.conv4 = Conv2d(256, 96, kernel_size=(3, 3), stride=1, padding=1)\n",
        "\t\tself.pool3 = MaxPool2d(kernel_size=(3,3), stride=2)\n",
        "\t\tself.dropout2 = Dropout(0.3)\n",
        "\t\tself.fc1 = Linear(25 * 17 * 96, 1024)\n",
        "\t\tself.dropout3 = Dropout(0.5)\n",
        "\t\tself.fc2 = Linear(1024, 128)\n",
        "\n",
        "\tdef forward_once(self, x):\n",
        "\t\tx = relu(self.conv1(x))\n",
        "\t\tx = self.lrn1(x)\n",
        "\t\tx = self.pool1(x)\n",
        "\t\tx = relu(self.conv2(x))\n",
        "\t\tx = self.lrn2(x)\n",
        "\t\tx = self.pool2(x)\n",
        "\t\tx = self.dropout1(x)\n",
        "\t\tx = relu(self.conv3(x))\n",
        "\t\tx = relu(self.conv4(x))\n",
        "\t\tx = self.pool3(x)\n",
        "\t\tx = self.dropout2(x)\n",
        "\t\tx = x.view(-1, 17 * 25 * 96)\n",
        "\t\tx = relu(self.fc1(x))\n",
        "\t\tx = self.dropout3(x)\n",
        "\t\tx = relu(self.fc2(x))\n",
        "\t\treturn x\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\tf_x = self.forward_once(x)\n",
        "\t\tf_y = self.forward_once(y)\n",
        "\t\treturn f_x, f_y\n",
        "\n",
        "\n",
        "def distance_metric(features_A, features_B):\n",
        "\tbatch_losses = F.pairwise_distance(features_A, features_B)\n",
        "\treturn batch_losses\n",
        "\n",
        "\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "\n",
        "\tdef __init__(self, margin=2.0):\n",
        "\t\tsuper(ContrastiveLoss, self).__init__()\n",
        "\t\tself.margin = margin\n",
        "\n",
        "\tdef forward(self, output1, output2, label):\n",
        "\t\teuclidean_distance = F.pairwise_distance(output1, output2)\n",
        "\t\tloss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
        "\t\t\t\t\t\t\t\t\t  (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
        "\n",
        "\t\treturn loss_contrastive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKHr08jkgYq",
        "colab_type": "code",
        "outputId": "f7dfc1c7-4889-4b30-ccb5-65e4092d7423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11132
        }
      },
      "source": [
        "model = SiameseConvNet().cuda()\n",
        "criterion = ContrastiveLoss().cuda()\n",
        "optimizer = Adam(model.parameters())\n",
        "\n",
        "train_dataset = TrainDataset()\n",
        "train_loader = DataLoader(train_dataset, batch_size=48, shuffle=False)\n",
        "\n",
        "\n",
        "def checkpoint(epoch):\n",
        "\tfile_path = \"model_epoch_%d\" % epoch\n",
        "\twith open(file_path, 'wb') as f:\n",
        "\t\tsave(model.state_dict(), f)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "\tfor batch_index, data in enumerate(train_loader):\n",
        "\t\tA = data[0].cuda()\n",
        "\t\tB = data[1].cuda()\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tlabel = data[2].float().cuda()\n",
        "\t\tf_A, f_B = model.forward(A, B)\n",
        "\t\tloss = criterion(f_A, f_B, label)\n",
        "\t\tprint('Epoch {}, batch {}, loss={}'.format(epoch, batch_index, loss.item()))\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\n",
        "for e in range(1, 21):\n",
        "\ttrain(e)\n",
        "\tcheckpoint(e)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, batch 0, loss=1.729467511177063\n",
            "Epoch 1, batch 1, loss=102.30181121826172\n",
            "Epoch 1, batch 2, loss=1.2160407304763794\n",
            "Epoch 1, batch 3, loss=1.0972392559051514\n",
            "Epoch 1, batch 4, loss=1.3364653587341309\n",
            "Epoch 1, batch 5, loss=1.2170413732528687\n",
            "Epoch 1, batch 6, loss=1.531886100769043\n",
            "Epoch 1, batch 7, loss=1.0243122577667236\n",
            "Epoch 1, batch 8, loss=1.1270262002944946\n",
            "Epoch 1, batch 9, loss=1.1037466526031494\n",
            "Epoch 1, batch 10, loss=1.092578649520874\n",
            "Epoch 1, batch 11, loss=1.0836076736450195\n",
            "Epoch 1, batch 12, loss=1.0571870803833008\n",
            "Epoch 1, batch 13, loss=1.2949249744415283\n",
            "Epoch 1, batch 14, loss=0.933813750743866\n",
            "Epoch 1, batch 15, loss=1.0925668478012085\n",
            "Epoch 1, batch 16, loss=1.0714420080184937\n",
            "Epoch 1, batch 17, loss=0.976224422454834\n",
            "Epoch 1, batch 18, loss=1.0472890138626099\n",
            "Epoch 1, batch 19, loss=1.1671735048294067\n",
            "Epoch 1, batch 20, loss=0.9847795963287354\n",
            "Epoch 1, batch 21, loss=1.1313570737838745\n",
            "Epoch 1, batch 22, loss=1.1070460081100464\n",
            "Epoch 1, batch 23, loss=1.0159920454025269\n",
            "Epoch 1, batch 24, loss=0.9721962809562683\n",
            "Epoch 1, batch 25, loss=1.0036884546279907\n",
            "Epoch 1, batch 26, loss=0.9896426200866699\n",
            "Epoch 1, batch 27, loss=1.039160966873169\n",
            "Epoch 1, batch 28, loss=1.0099096298217773\n",
            "Epoch 1, batch 29, loss=0.960237443447113\n",
            "Epoch 1, batch 30, loss=1.0074268579483032\n",
            "Epoch 1, batch 31, loss=0.9927740693092346\n",
            "Epoch 2, batch 0, loss=1.0389608144760132\n",
            "Epoch 2, batch 1, loss=1.0186809301376343\n",
            "Epoch 2, batch 2, loss=1.0242348909378052\n",
            "Epoch 2, batch 3, loss=1.0542203187942505\n",
            "Epoch 2, batch 4, loss=0.9781113266944885\n",
            "Epoch 2, batch 5, loss=1.0383700132369995\n",
            "Epoch 2, batch 6, loss=1.0114532709121704\n",
            "Epoch 2, batch 7, loss=0.9636014103889465\n",
            "Epoch 2, batch 8, loss=1.0141421556472778\n",
            "Epoch 2, batch 9, loss=1.0010384321212769\n",
            "Epoch 2, batch 10, loss=0.972688615322113\n",
            "Epoch 2, batch 11, loss=0.9953736662864685\n",
            "Epoch 2, batch 12, loss=0.9652573466300964\n",
            "Epoch 2, batch 13, loss=0.9601369500160217\n",
            "Epoch 2, batch 14, loss=1.1928889751434326\n",
            "Epoch 2, batch 15, loss=1.0542858839035034\n",
            "Epoch 2, batch 16, loss=1.084114670753479\n",
            "Epoch 2, batch 17, loss=1.0449268817901611\n",
            "Epoch 2, batch 18, loss=0.9486103653907776\n",
            "Epoch 2, batch 19, loss=0.9735813736915588\n",
            "Epoch 2, batch 20, loss=1.0447959899902344\n",
            "Epoch 2, batch 21, loss=1.0363423824310303\n",
            "Epoch 2, batch 22, loss=1.0497448444366455\n",
            "Epoch 2, batch 23, loss=1.0178338289260864\n",
            "Epoch 2, batch 24, loss=1.0647411346435547\n",
            "Epoch 2, batch 25, loss=1.0627566576004028\n",
            "Epoch 2, batch 26, loss=0.9977207183837891\n",
            "Epoch 2, batch 27, loss=1.0331907272338867\n",
            "Epoch 2, batch 28, loss=1.0261280536651611\n",
            "Epoch 2, batch 29, loss=0.856299638748169\n",
            "Epoch 2, batch 30, loss=1.0446369647979736\n",
            "Epoch 2, batch 31, loss=1.0890711545944214\n",
            "Epoch 3, batch 0, loss=0.9813732504844666\n",
            "Epoch 3, batch 1, loss=1.0974847078323364\n",
            "Epoch 3, batch 2, loss=0.9642519354820251\n",
            "Epoch 3, batch 3, loss=0.9258543848991394\n",
            "Epoch 3, batch 4, loss=0.9719775319099426\n",
            "Epoch 3, batch 5, loss=1.0173438787460327\n",
            "Epoch 3, batch 6, loss=0.9341828227043152\n",
            "Epoch 3, batch 7, loss=0.8632660508155823\n",
            "Epoch 3, batch 8, loss=1.0421379804611206\n",
            "Epoch 3, batch 9, loss=1.011404037475586\n",
            "Epoch 3, batch 10, loss=0.9745422005653381\n",
            "Epoch 3, batch 11, loss=0.9760592579841614\n",
            "Epoch 3, batch 12, loss=1.060294508934021\n",
            "Epoch 3, batch 13, loss=1.0180014371871948\n",
            "Epoch 3, batch 14, loss=1.0522960424423218\n",
            "Epoch 3, batch 15, loss=1.0344493389129639\n",
            "Epoch 3, batch 16, loss=0.966253936290741\n",
            "Epoch 3, batch 17, loss=1.0317858457565308\n",
            "Epoch 3, batch 18, loss=0.9659493565559387\n",
            "Epoch 3, batch 19, loss=1.0070438385009766\n",
            "Epoch 3, batch 20, loss=0.990896999835968\n",
            "Epoch 3, batch 21, loss=0.9524412155151367\n",
            "Epoch 3, batch 22, loss=0.9585099816322327\n",
            "Epoch 3, batch 23, loss=0.9543762803077698\n",
            "Epoch 3, batch 24, loss=0.9906213879585266\n",
            "Epoch 3, batch 25, loss=0.9567652344703674\n",
            "Epoch 3, batch 26, loss=1.0183664560317993\n",
            "Epoch 3, batch 27, loss=0.9375836253166199\n",
            "Epoch 3, batch 28, loss=1.0122512578964233\n",
            "Epoch 3, batch 29, loss=0.9423819184303284\n",
            "Epoch 3, batch 30, loss=0.9781575202941895\n",
            "Epoch 3, batch 31, loss=1.0133146047592163\n",
            "Epoch 4, batch 0, loss=1.003597617149353\n",
            "Epoch 4, batch 1, loss=1.098247766494751\n",
            "Epoch 4, batch 2, loss=0.8977721333503723\n",
            "Epoch 4, batch 3, loss=0.9617848992347717\n",
            "Epoch 4, batch 4, loss=0.9987906813621521\n",
            "Epoch 4, batch 5, loss=0.8782873153686523\n",
            "Epoch 4, batch 6, loss=1.0210081338882446\n",
            "Epoch 4, batch 7, loss=0.981719970703125\n",
            "Epoch 4, batch 8, loss=0.9638283848762512\n",
            "Epoch 4, batch 9, loss=0.986440122127533\n",
            "Epoch 4, batch 10, loss=0.9702186584472656\n",
            "Epoch 4, batch 11, loss=0.9391608834266663\n",
            "Epoch 4, batch 12, loss=1.03093683719635\n",
            "Epoch 4, batch 13, loss=0.9785349369049072\n",
            "Epoch 4, batch 14, loss=1.2314414978027344\n",
            "Epoch 4, batch 15, loss=1.0671392679214478\n",
            "Epoch 4, batch 16, loss=1.0163902044296265\n",
            "Epoch 4, batch 17, loss=1.0297521352767944\n",
            "Epoch 4, batch 18, loss=1.0514010190963745\n",
            "Epoch 4, batch 19, loss=1.0079243183135986\n",
            "Epoch 4, batch 20, loss=0.9409140944480896\n",
            "Epoch 4, batch 21, loss=0.926169216632843\n",
            "Epoch 4, batch 22, loss=0.9894316792488098\n",
            "Epoch 4, batch 23, loss=0.9901975989341736\n",
            "Epoch 4, batch 24, loss=0.9644133448600769\n",
            "Epoch 4, batch 25, loss=0.9769234657287598\n",
            "Epoch 4, batch 26, loss=1.0550347566604614\n",
            "Epoch 4, batch 27, loss=1.0373477935791016\n",
            "Epoch 4, batch 28, loss=0.9558860659599304\n",
            "Epoch 4, batch 29, loss=0.8630794882774353\n",
            "Epoch 4, batch 30, loss=1.011780858039856\n",
            "Epoch 4, batch 31, loss=1.095440149307251\n",
            "Epoch 5, batch 0, loss=0.9419137835502625\n",
            "Epoch 5, batch 1, loss=1.0879502296447754\n",
            "Epoch 5, batch 2, loss=0.9980069994926453\n",
            "Epoch 5, batch 3, loss=1.0119823217391968\n",
            "Epoch 5, batch 4, loss=0.9420590400695801\n",
            "Epoch 5, batch 5, loss=0.9149152636528015\n",
            "Epoch 5, batch 6, loss=0.9467548727989197\n",
            "Epoch 5, batch 7, loss=1.0620087385177612\n",
            "Epoch 5, batch 8, loss=0.9349038600921631\n",
            "Epoch 5, batch 9, loss=1.0085954666137695\n",
            "Epoch 5, batch 10, loss=1.0074299573898315\n",
            "Epoch 5, batch 11, loss=0.9349329471588135\n",
            "Epoch 5, batch 12, loss=1.0032988786697388\n",
            "Epoch 5, batch 13, loss=0.9601412415504456\n",
            "Epoch 5, batch 14, loss=1.07614266872406\n",
            "Epoch 5, batch 15, loss=0.8995779156684875\n",
            "Epoch 5, batch 16, loss=0.8969767689704895\n",
            "Epoch 5, batch 17, loss=0.988716185092926\n",
            "Epoch 5, batch 18, loss=0.9448410868644714\n",
            "Epoch 5, batch 19, loss=0.94809490442276\n",
            "Epoch 5, batch 20, loss=1.0638023614883423\n",
            "Epoch 5, batch 21, loss=0.9679433703422546\n",
            "Epoch 5, batch 22, loss=1.000353455543518\n",
            "Epoch 5, batch 23, loss=1.0189266204833984\n",
            "Epoch 5, batch 24, loss=1.049613356590271\n",
            "Epoch 5, batch 25, loss=0.9738702178001404\n",
            "Epoch 5, batch 26, loss=0.8985056281089783\n",
            "Epoch 5, batch 27, loss=1.0193986892700195\n",
            "Epoch 5, batch 28, loss=1.0113917589187622\n",
            "Epoch 5, batch 29, loss=0.8626396656036377\n",
            "Epoch 5, batch 30, loss=1.0331274271011353\n",
            "Epoch 5, batch 31, loss=0.9894607067108154\n",
            "Epoch 6, batch 0, loss=1.0492404699325562\n",
            "Epoch 6, batch 1, loss=1.0728012323379517\n",
            "Epoch 6, batch 2, loss=1.0382121801376343\n",
            "Epoch 6, batch 3, loss=0.9446902275085449\n",
            "Epoch 6, batch 4, loss=1.009076476097107\n",
            "Epoch 6, batch 5, loss=0.9488639831542969\n",
            "Epoch 6, batch 6, loss=1.001797080039978\n",
            "Epoch 6, batch 7, loss=0.9790056347846985\n",
            "Epoch 6, batch 8, loss=1.0343610048294067\n",
            "Epoch 6, batch 9, loss=1.0039514303207397\n",
            "Epoch 6, batch 10, loss=1.0232349634170532\n",
            "Epoch 6, batch 11, loss=1.0088592767715454\n",
            "Epoch 6, batch 12, loss=1.0473575592041016\n",
            "Epoch 6, batch 13, loss=1.0240156650543213\n",
            "Epoch 6, batch 14, loss=1.135044813156128\n",
            "Epoch 6, batch 15, loss=0.9235110282897949\n",
            "Epoch 6, batch 16, loss=0.9510132670402527\n",
            "Epoch 6, batch 17, loss=0.9556407332420349\n",
            "Epoch 6, batch 18, loss=0.9849185347557068\n",
            "Epoch 6, batch 19, loss=0.9403934478759766\n",
            "Epoch 6, batch 20, loss=0.9753567576408386\n",
            "Epoch 6, batch 21, loss=1.0304557085037231\n",
            "Epoch 6, batch 22, loss=1.029183030128479\n",
            "Epoch 6, batch 23, loss=0.9618301391601562\n",
            "Epoch 6, batch 24, loss=1.0334709882736206\n",
            "Epoch 6, batch 25, loss=0.9951055645942688\n",
            "Epoch 6, batch 26, loss=0.9469788670539856\n",
            "Epoch 6, batch 27, loss=1.0404579639434814\n",
            "Epoch 6, batch 28, loss=1.0853475332260132\n",
            "Epoch 6, batch 29, loss=0.865623414516449\n",
            "Epoch 6, batch 30, loss=1.0373127460479736\n",
            "Epoch 6, batch 31, loss=1.0021756887435913\n",
            "Epoch 7, batch 0, loss=1.0071462392807007\n",
            "Epoch 7, batch 1, loss=1.0916601419448853\n",
            "Epoch 7, batch 2, loss=0.8849676251411438\n",
            "Epoch 7, batch 3, loss=0.9559387564659119\n",
            "Epoch 7, batch 4, loss=0.9355809092521667\n",
            "Epoch 7, batch 5, loss=0.9423425197601318\n",
            "Epoch 7, batch 6, loss=1.034409999847412\n",
            "Epoch 7, batch 7, loss=0.9817200303077698\n",
            "Epoch 7, batch 8, loss=0.9964414238929749\n",
            "Epoch 7, batch 9, loss=0.9889646172523499\n",
            "Epoch 7, batch 10, loss=0.995688259601593\n",
            "Epoch 7, batch 11, loss=1.020147681236267\n",
            "Epoch 7, batch 12, loss=0.9913341999053955\n",
            "Epoch 7, batch 13, loss=0.9596793055534363\n",
            "Epoch 7, batch 14, loss=1.0860854387283325\n",
            "Epoch 7, batch 15, loss=1.0014513731002808\n",
            "Epoch 7, batch 16, loss=0.8948445916175842\n",
            "Epoch 7, batch 17, loss=1.0541353225708008\n",
            "Epoch 7, batch 18, loss=1.0223660469055176\n",
            "Epoch 7, batch 19, loss=0.942345142364502\n",
            "Epoch 7, batch 20, loss=1.0214152336120605\n",
            "Epoch 7, batch 21, loss=0.9656894207000732\n",
            "Epoch 7, batch 22, loss=1.0384994745254517\n",
            "Epoch 7, batch 23, loss=1.0181565284729004\n",
            "Epoch 7, batch 24, loss=1.047448992729187\n",
            "Epoch 7, batch 25, loss=1.037624478340149\n",
            "Epoch 7, batch 26, loss=0.92420893907547\n",
            "Epoch 7, batch 27, loss=0.9499969482421875\n",
            "Epoch 7, batch 28, loss=0.9717972874641418\n",
            "Epoch 7, batch 29, loss=0.9422809481620789\n",
            "Epoch 7, batch 30, loss=1.0497345924377441\n",
            "Epoch 7, batch 31, loss=1.0617554187774658\n",
            "Epoch 8, batch 0, loss=1.0360356569290161\n",
            "Epoch 8, batch 1, loss=1.084636926651001\n",
            "Epoch 8, batch 2, loss=0.934252917766571\n",
            "Epoch 8, batch 3, loss=0.877089262008667\n",
            "Epoch 8, batch 4, loss=0.9970940947532654\n",
            "Epoch 8, batch 5, loss=0.9256954193115234\n",
            "Epoch 8, batch 6, loss=1.0140472650527954\n",
            "Epoch 8, batch 7, loss=0.9660469889640808\n",
            "Epoch 8, batch 8, loss=1.002850890159607\n",
            "Epoch 8, batch 9, loss=0.9162283539772034\n",
            "Epoch 8, batch 10, loss=0.9714562892913818\n",
            "Epoch 8, batch 11, loss=0.9988400340080261\n",
            "Epoch 8, batch 12, loss=1.0321046113967896\n",
            "Epoch 8, batch 13, loss=0.8898393511772156\n",
            "Epoch 8, batch 14, loss=1.0943230390548706\n",
            "Epoch 8, batch 15, loss=0.9025686383247375\n",
            "Epoch 8, batch 16, loss=1.035375952720642\n",
            "Epoch 8, batch 17, loss=1.0078606605529785\n",
            "Epoch 8, batch 18, loss=0.9381652474403381\n",
            "Epoch 8, batch 19, loss=0.9550397992134094\n",
            "Epoch 8, batch 20, loss=0.9605617523193359\n",
            "Epoch 8, batch 21, loss=0.9329293370246887\n",
            "Epoch 8, batch 22, loss=1.017142415046692\n",
            "Epoch 8, batch 23, loss=0.9913483262062073\n",
            "Epoch 8, batch 24, loss=1.0479843616485596\n",
            "Epoch 8, batch 25, loss=1.0285212993621826\n",
            "Epoch 8, batch 26, loss=0.9980974197387695\n",
            "Epoch 8, batch 27, loss=0.9561415314674377\n",
            "Epoch 8, batch 28, loss=1.018640160560608\n",
            "Epoch 8, batch 29, loss=0.9130585789680481\n",
            "Epoch 8, batch 30, loss=0.9815219044685364\n",
            "Epoch 8, batch 31, loss=1.0095436573028564\n",
            "Epoch 9, batch 0, loss=0.8647108674049377\n",
            "Epoch 9, batch 1, loss=1.0255818367004395\n",
            "Epoch 9, batch 2, loss=0.9124941825866699\n",
            "Epoch 9, batch 3, loss=1.0033270120620728\n",
            "Epoch 9, batch 4, loss=0.9220671653747559\n",
            "Epoch 9, batch 5, loss=0.9148872494697571\n",
            "Epoch 9, batch 6, loss=1.047885775566101\n",
            "Epoch 9, batch 7, loss=0.9043951034545898\n",
            "Epoch 9, batch 8, loss=0.9896290302276611\n",
            "Epoch 9, batch 9, loss=1.0578227043151855\n",
            "Epoch 9, batch 10, loss=1.0403329133987427\n",
            "Epoch 9, batch 11, loss=0.9603607058525085\n",
            "Epoch 9, batch 12, loss=0.9704944491386414\n",
            "Epoch 9, batch 13, loss=1.0274842977523804\n",
            "Epoch 9, batch 14, loss=1.0252152681350708\n",
            "Epoch 9, batch 15, loss=0.9437554478645325\n",
            "Epoch 9, batch 16, loss=0.99578458070755\n",
            "Epoch 9, batch 17, loss=0.8805058598518372\n",
            "Epoch 9, batch 18, loss=0.9855647683143616\n",
            "Epoch 9, batch 19, loss=1.0097168684005737\n",
            "Epoch 9, batch 20, loss=0.9639577865600586\n",
            "Epoch 9, batch 21, loss=0.9374662041664124\n",
            "Epoch 9, batch 22, loss=1.036573886871338\n",
            "Epoch 9, batch 23, loss=0.9875946044921875\n",
            "Epoch 9, batch 24, loss=1.003998041152954\n",
            "Epoch 9, batch 25, loss=1.0424295663833618\n",
            "Epoch 9, batch 26, loss=0.9187007546424866\n",
            "Epoch 9, batch 27, loss=1.020426630973816\n",
            "Epoch 9, batch 28, loss=1.0840977430343628\n",
            "Epoch 9, batch 29, loss=0.9084057807922363\n",
            "Epoch 9, batch 30, loss=0.9554283618927002\n",
            "Epoch 9, batch 31, loss=1.0333988666534424\n",
            "Epoch 10, batch 0, loss=0.9578755497932434\n",
            "Epoch 10, batch 1, loss=1.0882152318954468\n",
            "Epoch 10, batch 2, loss=0.9152550101280212\n",
            "Epoch 10, batch 3, loss=0.9078381657600403\n",
            "Epoch 10, batch 4, loss=0.9331459999084473\n",
            "Epoch 10, batch 5, loss=0.9699239730834961\n",
            "Epoch 10, batch 6, loss=1.0086586475372314\n",
            "Epoch 10, batch 7, loss=0.9809854626655579\n",
            "Epoch 10, batch 8, loss=1.1101648807525635\n",
            "Epoch 10, batch 9, loss=0.928835391998291\n",
            "Epoch 10, batch 10, loss=1.0074540376663208\n",
            "Epoch 10, batch 11, loss=1.0106168985366821\n",
            "Epoch 10, batch 12, loss=0.9905433654785156\n",
            "Epoch 10, batch 13, loss=0.9519266486167908\n",
            "Epoch 10, batch 14, loss=1.0957536697387695\n",
            "Epoch 10, batch 15, loss=0.9742729067802429\n",
            "Epoch 10, batch 16, loss=1.0352035760879517\n",
            "Epoch 10, batch 17, loss=0.923253059387207\n",
            "Epoch 10, batch 18, loss=0.9863473773002625\n",
            "Epoch 10, batch 19, loss=0.9679421782493591\n",
            "Epoch 10, batch 20, loss=0.8465216159820557\n",
            "Epoch 10, batch 21, loss=0.9592623114585876\n",
            "Epoch 10, batch 22, loss=1.03756582736969\n",
            "Epoch 10, batch 23, loss=0.9089649319648743\n",
            "Epoch 10, batch 24, loss=0.9167697429656982\n",
            "Epoch 10, batch 25, loss=1.025060772895813\n",
            "Epoch 10, batch 26, loss=0.9598786234855652\n",
            "Epoch 10, batch 27, loss=0.8763169646263123\n",
            "Epoch 10, batch 28, loss=1.0710989236831665\n",
            "Epoch 10, batch 29, loss=0.8472854495048523\n",
            "Epoch 10, batch 30, loss=1.0624330043792725\n",
            "Epoch 10, batch 31, loss=0.9886500835418701\n",
            "Epoch 11, batch 0, loss=0.921671450138092\n",
            "Epoch 11, batch 1, loss=1.04909086227417\n",
            "Epoch 11, batch 2, loss=0.9355915188789368\n",
            "Epoch 11, batch 3, loss=0.9141299724578857\n",
            "Epoch 11, batch 4, loss=1.009013295173645\n",
            "Epoch 11, batch 5, loss=0.9303577542304993\n",
            "Epoch 11, batch 6, loss=1.05941641330719\n",
            "Epoch 11, batch 7, loss=0.9944527745246887\n",
            "Epoch 11, batch 8, loss=1.0072866678237915\n",
            "Epoch 11, batch 9, loss=0.9474228024482727\n",
            "Epoch 11, batch 10, loss=1.0035300254821777\n",
            "Epoch 11, batch 11, loss=0.9873008728027344\n",
            "Epoch 11, batch 12, loss=1.0057445764541626\n",
            "Epoch 11, batch 13, loss=0.9477813839912415\n",
            "Epoch 11, batch 14, loss=1.0544534921646118\n",
            "Epoch 11, batch 15, loss=0.9419637322425842\n",
            "Epoch 11, batch 16, loss=0.9816620349884033\n",
            "Epoch 11, batch 17, loss=0.9739306569099426\n",
            "Epoch 11, batch 18, loss=1.00467848777771\n",
            "Epoch 11, batch 19, loss=0.9644887447357178\n",
            "Epoch 11, batch 20, loss=0.9462862610816956\n",
            "Epoch 11, batch 21, loss=1.023148775100708\n",
            "Epoch 11, batch 22, loss=1.060034155845642\n",
            "Epoch 11, batch 23, loss=1.026851773262024\n",
            "Epoch 11, batch 24, loss=0.9530383944511414\n",
            "Epoch 11, batch 25, loss=0.9466946125030518\n",
            "Epoch 11, batch 26, loss=0.9879347681999207\n",
            "Epoch 11, batch 27, loss=1.0292489528656006\n",
            "Epoch 11, batch 28, loss=1.0442978143692017\n",
            "Epoch 11, batch 29, loss=0.8132200837135315\n",
            "Epoch 11, batch 30, loss=1.0739995241165161\n",
            "Epoch 11, batch 31, loss=1.0025376081466675\n",
            "Epoch 12, batch 0, loss=0.9786557555198669\n",
            "Epoch 12, batch 1, loss=1.0600506067276\n",
            "Epoch 12, batch 2, loss=0.9789177775382996\n",
            "Epoch 12, batch 3, loss=0.9045839309692383\n",
            "Epoch 12, batch 4, loss=0.9838600754737854\n",
            "Epoch 12, batch 5, loss=0.8826869130134583\n",
            "Epoch 12, batch 6, loss=1.0065044164657593\n",
            "Epoch 12, batch 7, loss=0.8742212653160095\n",
            "Epoch 12, batch 8, loss=0.9754898548126221\n",
            "Epoch 12, batch 9, loss=1.0007768869400024\n",
            "Epoch 12, batch 10, loss=0.9645595550537109\n",
            "Epoch 12, batch 11, loss=0.8459693789482117\n",
            "Epoch 12, batch 12, loss=1.0734858512878418\n",
            "Epoch 12, batch 13, loss=0.9235245585441589\n",
            "Epoch 12, batch 14, loss=1.0087215900421143\n",
            "Epoch 12, batch 15, loss=0.9987104535102844\n",
            "Epoch 12, batch 16, loss=1.0246609449386597\n",
            "Epoch 12, batch 17, loss=1.0287048816680908\n",
            "Epoch 12, batch 18, loss=0.9983701705932617\n",
            "Epoch 12, batch 19, loss=0.9839382767677307\n",
            "Epoch 12, batch 20, loss=0.9506781101226807\n",
            "Epoch 12, batch 21, loss=0.9699597954750061\n",
            "Epoch 12, batch 22, loss=0.9950869083404541\n",
            "Epoch 12, batch 23, loss=0.9624244570732117\n",
            "Epoch 12, batch 24, loss=1.0945231914520264\n",
            "Epoch 12, batch 25, loss=1.0145089626312256\n",
            "Epoch 12, batch 26, loss=0.9433909058570862\n",
            "Epoch 12, batch 27, loss=0.952868640422821\n",
            "Epoch 12, batch 28, loss=0.9432302117347717\n",
            "Epoch 12, batch 29, loss=0.886834442615509\n",
            "Epoch 12, batch 30, loss=1.0202722549438477\n",
            "Epoch 12, batch 31, loss=1.0978330373764038\n",
            "Epoch 13, batch 0, loss=0.9837251305580139\n",
            "Epoch 13, batch 1, loss=1.0827282667160034\n",
            "Epoch 13, batch 2, loss=0.89243483543396\n",
            "Epoch 13, batch 3, loss=0.9431850910186768\n",
            "Epoch 13, batch 4, loss=0.9552664756774902\n",
            "Epoch 13, batch 5, loss=0.9759500622749329\n",
            "Epoch 13, batch 6, loss=0.9864194989204407\n",
            "Epoch 13, batch 7, loss=0.8716670870780945\n",
            "Epoch 13, batch 8, loss=1.0153728723526\n",
            "Epoch 13, batch 9, loss=0.8787750601768494\n",
            "Epoch 13, batch 10, loss=1.032338261604309\n",
            "Epoch 13, batch 11, loss=1.006630778312683\n",
            "Epoch 13, batch 12, loss=1.0051164627075195\n",
            "Epoch 13, batch 13, loss=0.9361807703971863\n",
            "Epoch 13, batch 14, loss=1.056469202041626\n",
            "Epoch 13, batch 15, loss=0.8889983296394348\n",
            "Epoch 13, batch 16, loss=0.9986676573753357\n",
            "Epoch 13, batch 17, loss=0.9249888062477112\n",
            "Epoch 13, batch 18, loss=0.9734426140785217\n",
            "Epoch 13, batch 19, loss=1.0031789541244507\n",
            "Epoch 13, batch 20, loss=0.9369387030601501\n",
            "Epoch 13, batch 21, loss=0.9044902324676514\n",
            "Epoch 13, batch 22, loss=0.977401077747345\n",
            "Epoch 13, batch 23, loss=1.0213446617126465\n",
            "Epoch 13, batch 24, loss=1.0368860960006714\n",
            "Epoch 13, batch 25, loss=1.0214489698410034\n",
            "Epoch 13, batch 26, loss=0.9314672946929932\n",
            "Epoch 13, batch 27, loss=0.9271306991577148\n",
            "Epoch 13, batch 28, loss=1.017160415649414\n",
            "Epoch 13, batch 29, loss=0.9035713076591492\n",
            "Epoch 13, batch 30, loss=0.9758738875389099\n",
            "Epoch 13, batch 31, loss=1.0393688678741455\n",
            "Epoch 14, batch 0, loss=0.9810523986816406\n",
            "Epoch 14, batch 1, loss=1.0832464694976807\n",
            "Epoch 14, batch 2, loss=0.9222314357757568\n",
            "Epoch 14, batch 3, loss=0.9660285115242004\n",
            "Epoch 14, batch 4, loss=0.9578108787536621\n",
            "Epoch 14, batch 5, loss=0.8907182216644287\n",
            "Epoch 14, batch 6, loss=0.9994926452636719\n",
            "Epoch 14, batch 7, loss=0.8960302472114563\n",
            "Epoch 14, batch 8, loss=1.0273152589797974\n",
            "Epoch 14, batch 9, loss=0.9758868217468262\n",
            "Epoch 14, batch 10, loss=0.925510585308075\n",
            "Epoch 14, batch 11, loss=0.9792892336845398\n",
            "Epoch 14, batch 12, loss=1.0579824447631836\n",
            "Epoch 14, batch 13, loss=0.865841805934906\n",
            "Epoch 14, batch 14, loss=1.0751241445541382\n",
            "Epoch 14, batch 15, loss=0.9496119022369385\n",
            "Epoch 14, batch 16, loss=0.9623188972473145\n",
            "Epoch 14, batch 17, loss=0.9197314381599426\n",
            "Epoch 14, batch 18, loss=0.9570183157920837\n",
            "Epoch 14, batch 19, loss=0.921971321105957\n",
            "Epoch 14, batch 20, loss=0.9023649096488953\n",
            "Epoch 14, batch 21, loss=0.9510361552238464\n",
            "Epoch 14, batch 22, loss=0.9087291359901428\n",
            "Epoch 14, batch 23, loss=1.0091840028762817\n",
            "Epoch 14, batch 24, loss=1.040939211845398\n",
            "Epoch 14, batch 25, loss=0.9022078514099121\n",
            "Epoch 14, batch 26, loss=0.9279904365539551\n",
            "Epoch 14, batch 27, loss=0.9498901963233948\n",
            "Epoch 14, batch 28, loss=1.0293933153152466\n",
            "Epoch 14, batch 29, loss=0.8177199959754944\n",
            "Epoch 14, batch 30, loss=1.0942362546920776\n",
            "Epoch 14, batch 31, loss=0.9818756580352783\n",
            "Epoch 15, batch 0, loss=0.9770364761352539\n",
            "Epoch 15, batch 1, loss=1.0901166200637817\n",
            "Epoch 15, batch 2, loss=0.9411284327507019\n",
            "Epoch 15, batch 3, loss=0.8758991360664368\n",
            "Epoch 15, batch 4, loss=0.9615426063537598\n",
            "Epoch 15, batch 5, loss=0.935967206954956\n",
            "Epoch 15, batch 6, loss=0.9382886290550232\n",
            "Epoch 15, batch 7, loss=0.9589733481407166\n",
            "Epoch 15, batch 8, loss=1.0239678621292114\n",
            "Epoch 15, batch 9, loss=0.9270372986793518\n",
            "Epoch 15, batch 10, loss=0.9046825766563416\n",
            "Epoch 15, batch 11, loss=0.9937142729759216\n",
            "Epoch 15, batch 12, loss=1.0726367235183716\n",
            "Epoch 15, batch 13, loss=0.904735267162323\n",
            "Epoch 15, batch 14, loss=1.0995427370071411\n",
            "Epoch 15, batch 15, loss=0.908360481262207\n",
            "Epoch 15, batch 16, loss=0.9163861870765686\n",
            "Epoch 15, batch 17, loss=0.9594619870185852\n",
            "Epoch 15, batch 18, loss=1.0476371049880981\n",
            "Epoch 15, batch 19, loss=0.9280632138252258\n",
            "Epoch 15, batch 20, loss=0.8712000250816345\n",
            "Epoch 15, batch 21, loss=0.9781029224395752\n",
            "Epoch 15, batch 22, loss=0.9205663204193115\n",
            "Epoch 15, batch 23, loss=1.047089695930481\n",
            "Epoch 15, batch 24, loss=0.9924008250236511\n",
            "Epoch 15, batch 25, loss=0.9445868134498596\n",
            "Epoch 15, batch 26, loss=0.9063224792480469\n",
            "Epoch 15, batch 27, loss=0.8810627460479736\n",
            "Epoch 15, batch 28, loss=0.9374101758003235\n",
            "Epoch 15, batch 29, loss=0.8666703701019287\n",
            "Epoch 15, batch 30, loss=1.040970802307129\n",
            "Epoch 15, batch 31, loss=0.9320050477981567\n",
            "Epoch 16, batch 0, loss=1.008947730064392\n",
            "Epoch 16, batch 1, loss=1.0284167528152466\n",
            "Epoch 16, batch 2, loss=0.9048922657966614\n",
            "Epoch 16, batch 3, loss=0.8663772940635681\n",
            "Epoch 16, batch 4, loss=0.8825728893280029\n",
            "Epoch 16, batch 5, loss=0.9274212718009949\n",
            "Epoch 16, batch 6, loss=1.0695158243179321\n",
            "Epoch 16, batch 7, loss=0.8819670081138611\n",
            "Epoch 16, batch 8, loss=1.156275749206543\n",
            "Epoch 16, batch 9, loss=0.9566102623939514\n",
            "Epoch 16, batch 10, loss=0.9322389960289001\n",
            "Epoch 16, batch 11, loss=0.9225810170173645\n",
            "Epoch 16, batch 12, loss=1.011478304862976\n",
            "Epoch 16, batch 13, loss=0.8797993063926697\n",
            "Epoch 16, batch 14, loss=1.177180290222168\n",
            "Epoch 16, batch 15, loss=0.8429556488990784\n",
            "Epoch 16, batch 16, loss=0.9062332510948181\n",
            "Epoch 16, batch 17, loss=0.930469274520874\n",
            "Epoch 16, batch 18, loss=0.9175863862037659\n",
            "Epoch 16, batch 19, loss=0.9890496134757996\n",
            "Epoch 16, batch 20, loss=0.8747028708457947\n",
            "Epoch 16, batch 21, loss=0.9130542278289795\n",
            "Epoch 16, batch 22, loss=0.9591650366783142\n",
            "Epoch 16, batch 23, loss=0.8735410571098328\n",
            "Epoch 16, batch 24, loss=0.9920685291290283\n",
            "Epoch 16, batch 25, loss=0.9431366324424744\n",
            "Epoch 16, batch 26, loss=0.9858302474021912\n",
            "Epoch 16, batch 27, loss=0.9203052520751953\n",
            "Epoch 16, batch 28, loss=1.0238337516784668\n",
            "Epoch 16, batch 29, loss=0.8237244486808777\n",
            "Epoch 16, batch 30, loss=1.033460021018982\n",
            "Epoch 16, batch 31, loss=0.9769499897956848\n",
            "Epoch 17, batch 0, loss=0.9487367272377014\n",
            "Epoch 17, batch 1, loss=1.1106698513031006\n",
            "Epoch 17, batch 2, loss=0.894386351108551\n",
            "Epoch 17, batch 3, loss=0.97944575548172\n",
            "Epoch 17, batch 4, loss=0.9344429969787598\n",
            "Epoch 17, batch 5, loss=0.8372572064399719\n",
            "Epoch 17, batch 6, loss=1.1166191101074219\n",
            "Epoch 17, batch 7, loss=0.8952856659889221\n",
            "Epoch 17, batch 8, loss=1.0315126180648804\n",
            "Epoch 17, batch 9, loss=0.9253966212272644\n",
            "Epoch 17, batch 10, loss=0.935882031917572\n",
            "Epoch 17, batch 11, loss=0.9734551310539246\n",
            "Epoch 17, batch 12, loss=0.9366089701652527\n",
            "Epoch 17, batch 13, loss=0.9111699461936951\n",
            "Epoch 17, batch 14, loss=1.0626684427261353\n",
            "Epoch 17, batch 15, loss=0.8973646759986877\n",
            "Epoch 17, batch 16, loss=0.9727792143821716\n",
            "Epoch 17, batch 17, loss=0.8825247287750244\n",
            "Epoch 17, batch 18, loss=0.899989128112793\n",
            "Epoch 17, batch 19, loss=0.9408197402954102\n",
            "Epoch 17, batch 20, loss=0.951793909072876\n",
            "Epoch 17, batch 21, loss=0.9342572689056396\n",
            "Epoch 17, batch 22, loss=1.0081230401992798\n",
            "Epoch 17, batch 23, loss=0.9662635326385498\n",
            "Epoch 17, batch 24, loss=0.9390577673912048\n",
            "Epoch 17, batch 25, loss=0.9814561009407043\n",
            "Epoch 17, batch 26, loss=0.8956118226051331\n",
            "Epoch 17, batch 27, loss=0.8766343593597412\n",
            "Epoch 17, batch 28, loss=0.9911494255065918\n",
            "Epoch 17, batch 29, loss=0.793703019618988\n",
            "Epoch 17, batch 30, loss=0.9863004684448242\n",
            "Epoch 17, batch 31, loss=1.0482569932937622\n",
            "Epoch 18, batch 0, loss=0.913252592086792\n",
            "Epoch 18, batch 1, loss=1.0636990070343018\n",
            "Epoch 18, batch 2, loss=0.955274760723114\n",
            "Epoch 18, batch 3, loss=0.9230635762214661\n",
            "Epoch 18, batch 4, loss=1.0431643724441528\n",
            "Epoch 18, batch 5, loss=0.9054965972900391\n",
            "Epoch 18, batch 6, loss=0.9585545659065247\n",
            "Epoch 18, batch 7, loss=0.9367349147796631\n",
            "Epoch 18, batch 8, loss=1.008365511894226\n",
            "Epoch 18, batch 9, loss=1.0267893075942993\n",
            "Epoch 18, batch 10, loss=0.9998974204063416\n",
            "Epoch 18, batch 11, loss=0.9117074608802795\n",
            "Epoch 18, batch 12, loss=1.0458544492721558\n",
            "Epoch 18, batch 13, loss=0.9409496784210205\n",
            "Epoch 18, batch 14, loss=1.0419647693634033\n",
            "Epoch 18, batch 15, loss=0.947750985622406\n",
            "Epoch 18, batch 16, loss=0.9781742095947266\n",
            "Epoch 18, batch 17, loss=0.8922849297523499\n",
            "Epoch 18, batch 18, loss=1.037075400352478\n",
            "Epoch 18, batch 19, loss=0.9885395169258118\n",
            "Epoch 18, batch 20, loss=0.9559350609779358\n",
            "Epoch 18, batch 21, loss=0.9189260601997375\n",
            "Epoch 18, batch 22, loss=0.9019672274589539\n",
            "Epoch 18, batch 23, loss=1.08543062210083\n",
            "Epoch 18, batch 24, loss=1.0786974430084229\n",
            "Epoch 18, batch 25, loss=0.9377461075782776\n",
            "Epoch 18, batch 26, loss=0.8710560202598572\n",
            "Epoch 18, batch 27, loss=1.0272737741470337\n",
            "Epoch 18, batch 28, loss=1.0212892293930054\n",
            "Epoch 18, batch 29, loss=0.8485632538795471\n",
            "Epoch 18, batch 30, loss=0.9493811726570129\n",
            "Epoch 18, batch 31, loss=0.9996956586837769\n",
            "Epoch 19, batch 0, loss=0.945559561252594\n",
            "Epoch 19, batch 1, loss=1.0775372982025146\n",
            "Epoch 19, batch 2, loss=0.8866114020347595\n",
            "Epoch 19, batch 3, loss=0.8942830562591553\n",
            "Epoch 19, batch 4, loss=1.0184624195098877\n",
            "Epoch 19, batch 5, loss=0.8889167904853821\n",
            "Epoch 19, batch 6, loss=0.8996874690055847\n",
            "Epoch 19, batch 7, loss=0.9353654980659485\n",
            "Epoch 19, batch 8, loss=1.0449066162109375\n",
            "Epoch 19, batch 9, loss=0.9375565648078918\n",
            "Epoch 19, batch 10, loss=0.9911021590232849\n",
            "Epoch 19, batch 11, loss=0.991424560546875\n",
            "Epoch 19, batch 12, loss=1.0460644960403442\n",
            "Epoch 19, batch 13, loss=0.9036856293678284\n",
            "Epoch 19, batch 14, loss=1.0647499561309814\n",
            "Epoch 19, batch 15, loss=0.8041136264801025\n",
            "Epoch 19, batch 16, loss=0.9930058121681213\n",
            "Epoch 19, batch 17, loss=0.90162593126297\n",
            "Epoch 19, batch 18, loss=0.9329504370689392\n",
            "Epoch 19, batch 19, loss=1.0059415102005005\n",
            "Epoch 19, batch 20, loss=0.9979972839355469\n",
            "Epoch 19, batch 21, loss=0.9530776143074036\n",
            "Epoch 19, batch 22, loss=1.0402261018753052\n",
            "Epoch 19, batch 23, loss=0.9620437026023865\n",
            "Epoch 19, batch 24, loss=0.9889617562294006\n",
            "Epoch 19, batch 25, loss=0.9966645836830139\n",
            "Epoch 19, batch 26, loss=0.9456754326820374\n",
            "Epoch 19, batch 27, loss=0.8889634609222412\n",
            "Epoch 19, batch 28, loss=1.1028016805648804\n",
            "Epoch 19, batch 29, loss=0.9575293660163879\n",
            "Epoch 19, batch 30, loss=1.0651382207870483\n",
            "Epoch 19, batch 31, loss=1.0238685607910156\n",
            "Epoch 20, batch 0, loss=1.0252556800842285\n",
            "Epoch 20, batch 1, loss=1.0695298910140991\n",
            "Epoch 20, batch 2, loss=0.8830692768096924\n",
            "Epoch 20, batch 3, loss=0.8729172348976135\n",
            "Epoch 20, batch 4, loss=0.9719274640083313\n",
            "Epoch 20, batch 5, loss=0.9597366452217102\n",
            "Epoch 20, batch 6, loss=0.9985435605049133\n",
            "Epoch 20, batch 7, loss=1.0229442119598389\n",
            "Epoch 20, batch 8, loss=1.0579954385757446\n",
            "Epoch 20, batch 9, loss=0.9823676943778992\n",
            "Epoch 20, batch 10, loss=1.034410834312439\n",
            "Epoch 20, batch 11, loss=0.9352839589118958\n",
            "Epoch 20, batch 12, loss=1.0009965896606445\n",
            "Epoch 20, batch 13, loss=0.9009978771209717\n",
            "Epoch 20, batch 14, loss=1.0526708364486694\n",
            "Epoch 20, batch 15, loss=0.938450813293457\n",
            "Epoch 20, batch 16, loss=0.9674636721611023\n",
            "Epoch 20, batch 17, loss=0.8973384499549866\n",
            "Epoch 20, batch 18, loss=0.9104251265525818\n",
            "Epoch 20, batch 19, loss=1.0663881301879883\n",
            "Epoch 20, batch 20, loss=0.9176905155181885\n",
            "Epoch 20, batch 21, loss=0.9207724928855896\n",
            "Epoch 20, batch 22, loss=1.0205262899398804\n",
            "Epoch 20, batch 23, loss=0.941988468170166\n",
            "Epoch 20, batch 24, loss=0.9251841902732849\n",
            "Epoch 20, batch 25, loss=0.979733407497406\n",
            "Epoch 20, batch 26, loss=0.8949096202850342\n",
            "Epoch 20, batch 27, loss=0.9047402739524841\n",
            "Epoch 20, batch 28, loss=1.05855393409729\n",
            "Epoch 20, batch 29, loss=0.8914133906364441\n",
            "Epoch 20, batch 30, loss=0.9659537672996521\n",
            "Epoch 20, batch 31, loss=0.8749013543128967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDxyunpxkgYt",
        "colab_type": "code",
        "outputId": "5ef46d41-5ab1-433f-8e30-cf208b8fdbce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-9d9488bc9777>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mf_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max accuracy for batch {} = {} at d={}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    }
  ]
}